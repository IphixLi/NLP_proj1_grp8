{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "terms = ['actor', 'actress', 'director']\n",
    "\n",
    "def check_validity(name, content):\n",
    "    for n in name.lower().split():\n",
    "        if content.lower().find(n) == -1:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def most_frequent_term(content, terms) -> list:\n",
    "    count_map = {term: content.lower().count(term) for term in terms}\n",
    "    counts = sorted(count_map.items(), key=lambda x: x[1], reverse=True)\n",
    "    max_count = counts[0][1]\n",
    "    \n",
    "    if max_count > 0:\n",
    "        most_frequent = [term for term, count in counts if count == max_count]\n",
    "    else:\n",
    "        most_frequent = []\n",
    "    return most_frequent\n",
    "\n",
    "def possible_job_list(name) -> list:\n",
    "    try:\n",
    "        print(f\"wikipedia: Searching for '{name}'\")\n",
    "        search_results = wikipedia.search(name)\n",
    "        if not search_results:\n",
    "            print(f\"wikipedia: No search results found for '{name}'\")\n",
    "            return []\n",
    "        \n",
    "        content = wikipedia.page(search_results[0], auto_suggest=False).content\n",
    "        if not check_validity(name, content):\n",
    "            return []\n",
    "\n",
    "        return most_frequent_term(content, terms)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(f\"wikipedia: Ambiguous. Possible matches include: {e.options} for '{name}'\")\n",
    "        return []\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print(f\"wikipedia: The page does not exist: '{name}'\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"wikipedia: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "from syntax_analysis import find_verbs, find_persons, find_work_of_art, generate_candidates, get_descendants_precise, get_descendants_idx, get_descendants_greedy, is_human_pronoun\n",
    "from data_preprocess import TweetsPreprocessor\n",
    "from vote import Vote\n",
    "from wiki import WikiSearch\n",
    "from timestamp_cluster import TimestampCluster\n",
    "from handle_names import WinnerNameMatcher, about_human, get_job, name_cleaning\n",
    "\n",
    "# TODO: change it to the correct folder\n",
    "winner_result_folder = \"winner_result\"\n",
    "winner_vote_file = \"vote_winner_verb.json\"\n",
    "winner_to_award = {data['winner']: award.lower() for award, data in json.load(open(\"gg2013answers.json\"))['award_data'].items()}\n",
    "winner_matcher = WinnerNameMatcher(winner_to_award)\n",
    "\n",
    "spacy_model = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "timestamp_cluster = TimestampCluster(load_saved=True)\n",
    "\n",
    "awards = json.load(open(\"gg2013answers.json\"))['award_data'].keys()\n",
    "\n",
    "def get_award_keywords(awards) -> dict:\n",
    "    keyword_to_awards = {}\n",
    "\n",
    "    for award in awards:\n",
    "        spacy_output = spacy_model(award)\n",
    "        for token in spacy_output:\n",
    "            # Check if the token is a stopword, punctuation, or short word\n",
    "            if token.is_stop or token.is_punct or len(token.text) < 3:\n",
    "                continue\n",
    "            \n",
    "            # Check if the token's part-of-speech is one of the desired categories\n",
    "            if token.pos_ in [\"ADJ\", \"NOUN\", \"ADV\", \"VERB\", \"PROPN\"]:\n",
    "                keyword = token.text.lower()\n",
    "\n",
    "                if keyword not in keyword_to_awards:\n",
    "                    keyword_to_awards[keyword] = [award]\n",
    "                else:\n",
    "                    keyword_to_awards[keyword].append(award)\n",
    "    del keyword_to_awards['award']\n",
    "    return keyword_to_awards\n",
    "\n",
    "keyword_to_awards = get_award_keywords(awards)\n",
    "award_keywords = list(keyword_to_awards.keys())\n",
    "\n",
    "nominee_ts_verbs = [\n",
    "    \"win\", \"receive\", \"get\", \"take\", \"rob\", \"be\"\n",
    "]\n",
    "nominee_active_verbs = [\n",
    "    \"win\", \"receive\", \"get\", \"take\", \"rob\"\n",
    "]\n",
    "nominee_passive_verbs = [\n",
    "    \"awarded\", \"go\"\n",
    "]\n",
    "\n",
    "# def remove_words_after_for(text: str) -> str:\n",
    "#     for_idx = text.find(\"for\")\n",
    "#     if for_idx == -1:\n",
    "#         return text\n",
    "#     return text[:for_idx].strip()\n",
    "\n",
    "# def subtract_120_seconds(timestamp_ms: int) -> int:\n",
    "#     return timestamp_ms - 120000\n",
    "\n",
    "# timestamp_list = json.load(open(\"winner_result/timestamp_winner_verb.json\"))\n",
    "# timestamp_to_award = {\n",
    "#     subtract_120_seconds(lst[1][1]): lst[0] for lst in timestamp_list\n",
    "# }\n",
    "# award_to_winner = {\n",
    "#     lst[0]: remove_words_after_for(lst[1][0]) for lst in timestamp_list\n",
    "# }\n",
    "\n",
    "def match_nominee_verb_pattern(text: str) -> bool:\n",
    "    # List of verb-based patterns\n",
    "    patterns = [\n",
    "        r'\\b(nominated for)\\b',\n",
    "        r'\\b((should|shld|would) have|should\\'ve|shld\\'ve|would\\'ve) (won|received|got|taken home|gone to|been awarded to|been\\b(?!.*\\bnominated\\b))\\b',\n",
    "        r'\\b(should|shld|will) (win|receive|get|take home|go to|be awarded to)\\b',\n",
    "        r'I (wish|hope|guess|think|bet|predict|expect) .*(wins|win|receives|receive|gets|get|goes to|go to|awarded to|take(s) home)\\b',\n",
    "        r'(want|wanted) .* to (win|receive|get|go to|be awarded to|take home|be\\b(?!.*\\bnominated\\b))\\b',\n",
    "        r'(would like|hoping) .* (wins|win|receives|receive|gets|get|goes to|go to|awarded to|take(|s) home)\\b',\n",
    "        r'\\b((was|is|got|get) robbed)\\b',\n",
    "    ]\n",
    "    \n",
    "    # Check each pattern against the text\n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def nominee_verb_based_match(tweet: dict, base_confidence=1.0):\n",
    "    \"\"\"\n",
    "    Given a tweet, find the nominee and award\n",
    "    \n",
    "    Requires: tweet['new_text'] includes nominee_verb_pattern\n",
    "    Modifies: tweet['candidates']\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spacy_output = tweet.get('spacy_output', spacy_model(tweet['new_text']))\n",
    "        \n",
    "        for sentence in spacy_output.sents:\n",
    "            # Y (should / will) win X / Y is nominated for X / Y gets robbed\n",
    "            verb_list = find_verbs(sentence)\n",
    "            for verb in verb_list:\n",
    "                root = verb\n",
    "                award = []\n",
    "                nominee = []\n",
    "                cur_confidence = base_confidence\n",
    "                \n",
    "                if root.lemma_ in nominee_active_verbs:\n",
    "                    for child in root.children:\n",
    "                        if child.dep_ == \"dobj\":\n",
    "                            award += generate_candidates(child, root)\n",
    "                        elif child.dep_ == \"nsubj\" or child.dep_ == \"nsubjpass\":\n",
    "                            if child.pos_ == \"PRON\":\n",
    "                                cur_confidence *= 0.8\n",
    "                                known, is_human = is_human_pronoun(child.text)\n",
    "                                if known:\n",
    "                                    if is_human:\n",
    "                                        nominee += [p.text for p in find_persons(spacy_output) if p.root.i <= child.i]\n",
    "                                    else:\n",
    "                                        nominee += [w.text for w in find_work_of_art(spacy_output) if w.root.i < child.i]\n",
    "                                else:\n",
    "                                    nominee += [p.text for p in find_persons(spacy_output) if p.root.i <= child.i]\n",
    "                                    nominee += [w.text for w in find_work_of_art(spacy_output) if w.root.i < child.i]\n",
    "                                    nominee = list(set(nominee))\n",
    "                                    cur_confidence *= 0.8\n",
    "                            else:\n",
    "                                nominee.append(get_descendants_precise(child))                    \n",
    "                # X (is awarded to / goes to) Y\n",
    "                elif root.lemma_ in nominee_passive_verbs and root.i + 1 < len(sentence) and sentence[root.i + 1].text == \"to\":                 \n",
    "                    for chunk in sentence.noun_chunks:\n",
    "                        if chunk.root.head == root and (chunk.root.dep_ == \"nsubj\" or chunk.root.dep_ == \"nsubjpass\"):\n",
    "                            award.append(get_descendants_precise(chunk.root))\n",
    "                        elif chunk.root.dep_ == \"pobj\" and (chunk.root.head == root or chunk.root.head.text == \"to\"):\n",
    "                            if chunk.root.pos_ == \"PRON\":\n",
    "                                cur_confidence *= 0.8\n",
    "                                known, is_human = is_human_pronoun(chunk.root.text)\n",
    "                                if known:\n",
    "                                    if is_human:\n",
    "                                        nominee += [p.text for p in find_persons(spacy_output) if p.root.i <= child.i]\n",
    "                                    else:\n",
    "                                        nominee += [w.text for w in find_work_of_art(spacy_output) if w.root.i < child.i]\n",
    "                                else:\n",
    "                                    nominee += [p.text for p in find_persons(spacy_output) if p.root.i <= child.i]\n",
    "                                    nominee += [w.text for w in find_work_of_art(spacy_output) if w.root.i < child.i]\n",
    "                                    nominee = list(set(nominee))\n",
    "                                    cur_confidence *= 0.8\n",
    "                            else:\n",
    "                                idxs = get_descendants_idx(chunk.root)\n",
    "                                precise = get_descendants_precise(chunk.root)\n",
    "                                greedy = get_descendants_greedy(chunk.root, idxs, root)\n",
    "                                nominee += list(set([precise, greedy]))\n",
    "\n",
    "                if nominee:\n",
    "                    if not award:\n",
    "                        cur_confidence *= 0.5\n",
    "                        # do some inference based on keywords\n",
    "                        keyword_constraints = []\n",
    "                        for token in sentence:\n",
    "                            if token.text.lower() in award_keywords and not token.text.lower() in keyword_constraints:\n",
    "                                keyword_constraints.append(token.text.lower())\n",
    "                        \n",
    "                        # only move forward if there is at least one keyword\n",
    "                        if keyword_constraints:\n",
    "                            possible_awards = set(awards)\n",
    "                            for keyword in keyword_constraints:\n",
    "                                possible_awards = possible_awards.intersection(keyword_to_awards[keyword])\n",
    "\n",
    "                            if possible_awards:\n",
    "                                award = list(possible_awards)\n",
    "                                cur_confidence /= len(award)\n",
    "                    else:\n",
    "                        for token in sentence:\n",
    "                            if token.text.lower() in award_keywords:\n",
    "                                award.append(token.text.lower())\n",
    "                                \n",
    "        \n",
    "                if nominee or award:\n",
    "                    candidates = {\n",
    "                        \"nominee_candidates\": [name_cleaning(n) for n in list(set(nominee))],\n",
    "                        \"award_candidates\": list(set(award)),\n",
    "                        \"base_confidence\": cur_confidence,\n",
    "                    }\n",
    "                    tweet[\"candidates\"] = tweet.get(\"candidates\", []) + [candidates]\n",
    "        \n",
    "    except:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = {\n",
    "        \"text\": \"\\\"Allow us to present: Nepotism!\\\" -Eva Longoria and Don Cheadle #GoldenGlobes\",\n",
    "        \"user\": {\n",
    "            \"screen_name\": \"laleviner\",\n",
    "            \"id\": 75938926\n",
    "        },\n",
    "        \"id\": 290628849807523840,\n",
    "        \"timestamp_ms\": 1358124849000,\n",
    "        \"new_text\": \"\\\"Allow us to present: Nepotism!\\\" -Eva Longoria and Don Cheadle\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 0)\n",
      "('best performance by an actor in a supporting role in a motion picture', -3.252083333333333)\n",
      "2013-01-13 18:54:09\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from timestamp_cluster import TimestampCluster\n",
    "from datetime import datetime\n",
    "\n",
    "tc = TimestampCluster(load_saved=True)\n",
    "print(tc.categorize_timestamp(tweet['timestamp_ms']))\n",
    "print(tc.categorize_timestamp_after(tweet['timestamp_ms']))\n",
    "\n",
    "print(datetime.fromtimestamp(tweet['timestamp_ms'] / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/NU/2023/fall/337-natural_language_processing/projects/p1/yifan/nominee_check.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/NU/2023/fall/337-natural_language_processing/projects/p1/yifan/nominee_check.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m \u001b[39mimport\u001b[39;00m displacy\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/NU/2023/fall/337-natural_language_processing/projects/p1/yifan/nominee_check.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m doc \u001b[39m=\u001b[39m spacy_model(tweet[\u001b[39m'\u001b[39m\u001b[39mnew_text\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/NU/2023/fall/337-natural_language_processing/projects/p1/yifan/nominee_check.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# displacy.render(doc)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/NU/2023/fall/337-natural_language_processing/projects/p1/yifan/nominee_check.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m([(ent\u001b[39m.\u001b[39mtext, ent\u001b[39m.\u001b[39mlabel_) \u001b[39mfor\u001b[39;00m ent \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39ments])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from spacy import displacy\n",
    "\n",
    "doc = spacy_model(tweet['new_text'])\n",
    "# displacy.render(doc)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "if match_nominee_verb_pattern(tweet['new_text']):\n",
    "    nominee_verb_based_match(tweet, 1.0)\n",
    "\n",
    "print(json.dumps(tweet, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "def correct_dep(matcher, doc, i, matches):\n",
    "    _, start, end = matches[i]\n",
    "    for token in doc[start:end]:\n",
    "        if token.text == \"supporting\":\n",
    "            token.dep_ = \"amod\"  # set as an adjective modifier\n",
    "        elif token.text == \"actress\":\n",
    "            token.dep_ = \"nsubj\"  # set as the nominal subject\n",
    "\n",
    "# Create a Matcher pattern to identify the problematic structure\n",
    "matcher = Matcher(spacy_model.vocab)\n",
    "pattern = [{\"LOWER\": \"supporting\"}, {\"LOWER\": \"actress\"}]\n",
    "matcher.add(\"SUPPORTING_ACTRESS\", [pattern], on_match=correct_dep)\n",
    "\n",
    "winner_verb_extract(tweet)\n",
    "doc = spacy_model(tweet['new_text'])\n",
    "matcher(doc)\n",
    "displacy.render(doc, style=\"dep\", page=True, minify=True)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "print(json.dumps(tweet, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
